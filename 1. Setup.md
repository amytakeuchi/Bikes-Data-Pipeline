# Setup

## Setting up Google Cloud Platform
### Create GCP Account
Getting started with a GCP account is a straightforward and cost-free process, utilizing an existing Google or Gmail account. It's important to note that you have two options to choose from: the permanent free tier and the free trial.

The permanent free tier grants you continuous access to a specific amount of processing power for several key GCP services. Notably, it provides a substantial processing capacity for BigQuery, which is the primary tool employed by this pipeline to establish the main database.

Alternatively, there's the free trial option, which offers $300 in complimentary credits valid for 90 days following activation. The data processing demands of this pipeline are relatively modest, allowing you to execute it at minimal or no cost, even without leveraging the free trial. During the development of this pipeline, I consumed $XXXXXX in free trial credits. Given the extensive development iterations and debugging involved, it's probable that executing the pipeline once would incur minimal expenses. The primary expenditure was associated with the Dataproc cluster, which hosts Spark jobs for data transformation and loading into BigQuery.

### Create a GCP Project
Firstly, I created a Google Cloud Project (e.g. Bike-data-pipeline)
Next, I enable relevant APIs for the project, including that for Cloud Storage, Compute Engine(If you want to run a VM) , BigQuery, and Dataproc.
Lastly, we must establish service accounts with the necessary authorizations to utilize these APIs. Within IAM -> Service accounts, insert a new service account endowed with the "Viewer" role. You can then generate keys for this service account via the following steps: Actions -> Manage Keys -> AddKey -> Create new key -> Create. Remember to securely store this key on your local computer or VM, such as in ~/.google/credentials/, depending on where you intend to execute the pipeline.

To enable the service account to effectively interact with GCP storage, BigQuery, and Dataproc, it should be endowed with the following roles. Head over to IAM & Admin -> IAM, and proceed to edit the service account, assigning the roles listed below:

Viewer
Service Account User
Storage Admin
Storage Object Admin
BigQuery Admin
Dataproc Admin

## Set up environment - AirFlow
### (1) Install required packages - Docker and Terraform
Docker: First, you need to install Docker on your local machine or server. You can download Docker from the official website (https://www.docker.com/) and follow the installation instructions for your specific operating system.
Terraform: Similarly, you'll need to install Terraform on your machine. Visit the official Terraform website (https://www.terraform.io/downloads.html) and download the appropriate version for your OS. Follow the installation instructions to set up Terraform.
### (2) Setup envorinment variables
Configure any necessary environment variables for both Docker and Terraform. These variables might include authentication credentials, API keys, and other settings required for your specific project and cloud provider.
```
cd terraform
terraform init
terraform plan
terraform apply
```
- ** Locate the .env Example File: **
  In the top-level directory of your project repository, you will find a file named .env_example. This file serves as a template for your environment variables and contains placeholders for the necessary configuration details.
- Fill in the .env_example File:
  Open the .env_example file in a text editor of your choice.
You must provide specific information related to your project and data sources. This typically includes:
Google Cloud Project Details: Include the name of your Google Cloud project, the name of the storage bucket you'll be using, and the BigQuery dataset name. These details are essential for both the Dockerized environment and Terraform configuration.
CEDA Archive Account: If your project involves data from the CEDA archive, you'll need to register an account with CEDA. This account will allow you to access the relevant data hosted on their servers.
In the top-level directory of your project repository, you will find a file named .env_example. This file serves as a template for your environment variables and contains placeholders for the necessary configuration details.
- Rename the .env_example File:
  After filling in the required details in the .env_example file, rename it to .env. This is the naming convention commonly used for environment variable files.
- Add FTP Credentials:
  If your project requires access to data from the CEDA archive, you'll need to include your FTP username and password in the .env file.
These FTP credentials are used to authenticate and access data from the CEDA archive's FTP server.
### (3) Setup cloud resources on Terraform
Write Terraform configuration files (typically with a .tf extension) to define the infrastructure you want to provision on your cloud provider (e.g., AWS, Google Cloud, Azure). These files describe resources like virtual machines, networks, databases, and any other components needed for your Airflow setup.
Use the terraform init command to initialize your Terraform workspace.
Use the terraform plan command to preview the changes that Terraform will make to your infrastructure.
Finally, use the terraform apply command to create and provision the defined resources on your cloud provider.
### (4) Setting up Docker Compose
Create a Docker Compose configuration file (usually named docker-compose.yml) to define the services and containers needed for your Airflow environment.
Specify the Docker images for Airflow components (such as the web server, scheduler, worker, and database) in the Docker Compose file.
Use the docker-compose up command to start the Airflow containers. Docker Compose will create the specified containers and set up the network connections between them.
### (5) Setup  Airflow Settings
After starting Airflow using Docker Compose, you'll need to configure Airflow settings, including connections to your cloud provider, database, and any other services.
Access the Airflow web interface and use it to configure DAGs (Directed Acyclic Graphs) for your workflows and tasks.
